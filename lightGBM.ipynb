{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "train_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 국가를 대륙으로 매핑하는 딕셔너리\n",
    "country_continent_dict = {\n",
    " 'US': 'US',\n",
    " 'Cuba': 'North America',\n",
    " 'Portugal': 'Europe',\n",
    " 'Mexico': 'North America',\n",
    " 'Unknown': 'US',\n",
    " 'Puerto-Rico': 'North America',\n",
    " 'Germany': 'Europe',\n",
    " 'Japan': 'Asia',\n",
    " 'Poland': 'Europe',\n",
    " 'Columbia': 'South America',\n",
    " 'Philippines': 'Asia',\n",
    " 'Italy': 'Europe',\n",
    " 'Trinadad&Tobago': 'South America',\n",
    " 'England': 'Europe',\n",
    " 'South Korea': 'Asia',\n",
    " 'Iran': 'Asia',\n",
    " 'France': 'Europe',\n",
    " 'India': 'Asia',\n",
    " 'China': 'Asia',\n",
    " 'Dominican-Republic': 'North America',\n",
    " 'Scotland': 'Europe',\n",
    " 'Ecuador': 'South America',\n",
    " 'Nicaragua': 'North America',\n",
    " 'Peru': 'South America',\n",
    " 'Cambodia': 'Asia',\n",
    " 'Canada': 'North America',\n",
    " 'Jamaica': 'North America',\n",
    " 'Vietnam': 'Asia',\n",
    " 'Hong Kong': 'Asia',\n",
    " 'Thailand': 'Asia',\n",
    " 'Haiti': 'North America',\n",
    " 'Guatemala': 'North America',\n",
    " 'Laos': 'Asia',\n",
    " 'Yugoslavia': 'Europe',\n",
    " 'Ireland': 'Europe',\n",
    " 'El-Salvador': 'North America',\n",
    " 'Panama': 'North America',\n",
    " 'Honduras': 'North America',\n",
    " 'Greece': 'Europe',\n",
    " 'Outlying-U S (Guam USVI etc)': 'US',\n",
    " 'Hungary': 'Europe',\n",
    " 'Taiwan': 'Asia',\n",
    " 'Holand-Netherlands': 'Europe'\n",
    "}\n",
    "\n",
    "# 업데이트할 열 목록: 본인 출신국가, 엄마 출신국가, 아빠 출신국가\n",
    "columns_to_update = ['Birth_Country', 'Birth_Country (Mother)', 'Birth_Country (Father)']\n",
    "\n",
    "# 각 열에 대해 국가를 대륙으로 매핑\n",
    "for column in columns_to_update:\n",
    "    train_data[column] = train_data[column].map(country_continent_dict)\n",
    "\n",
    "# Remove rows where 'Age' is below 17 or above 75\n",
    "train_data = train_data[train_data['Age'].between(17, 75)]\n",
    "\n",
    "# Remove rows with 'Employment Status' as 'Not Working' or 'Seeking Full-Time'\n",
    "train_data = train_data[~train_data['Employment_Status'].isin(['Not Working', 'Seeking Full-Time'])]\n",
    "\n",
    "# Drop the 'Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status' columns\n",
    "train_data.drop(['Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status'], axis=1, inplace=True)\n",
    "\n",
    "# Map 'Gender' values from 'M' and 'F' to 0 and 1, respectively\n",
    "train_data['Gender'] = train_data['Gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "\n",
    "## 내가 추가 income이상치 제거\n",
    "train_data = train_data[train_data['Income'] <= 2500]\n",
    "\n",
    "\n",
    "# Consolidate education levels and rename as specified\n",
    "education_map = {\n",
    "    'High graduate': 'High', 'High Senior': 'High', \n",
    "    'High Junior': 'High', 'High Sophomore': 'High',\n",
    "    'Elementary (5-6)': 'Elementary(1-6)', 'Elementary (1-4)': 'Elementary(1-6)',\n",
    "    'Kindergarten': 'Baby', 'Children': 'Baby'\n",
    "}\n",
    "train_data['Education_Status'] = train_data['Education_Status'].replace(education_map)\n",
    "columns=[\n",
    "    'Education_Status',\n",
    "    'Employment_Status',\n",
    "    'Industry_Status',\n",
    "    'Occupation_Status',\n",
    "    'Race',\n",
    "    'Hispanic_Origin',\n",
    "    'Martial_Status',\n",
    "    'Household_Summary',\n",
    "    'Citizenship',\n",
    "    'Birth_Country',\n",
    "    'Birth_Country (Father)',\n",
    "    'Birth_Country (Mother)',\n",
    "    'Tax_Status'\n",
    "]\n",
    "edit_train_data=pd.get_dummies(train_data,columns=columns,dtype=int)\n",
    "edit_train_data = edit_train_data.drop(columns=['ID'])\n",
    "edit_train_data = edit_train_data.drop(columns=['Income'])\n",
    "edit_train_data.to_csv(\"editedTrain.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 335\n",
      "[LightGBM] [Info] Number of data points in the train set: 12528, number of used features: 113\n",
      "[LightGBM] [Info] Start training from score 646.940693\n",
      "Best Parameters: {'subsample': 1.0, 'num_leaves': 31, 'n_estimators': 200, 'min_child_samples': 10, 'max_depth': 9, 'learning_rate': 0.030538555088334154, 'colsample_bytree': 0.7}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "RMSE: 501.33312214654745\n",
      "R²: 0.25907719155029474\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 불러오기 (실제 데이터로 대체 필요)\n",
    "X = edit_train_data\n",
    "y = train_data['Income']\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM 모델 생성\n",
    "lgbm_model = LGBMRegressor()\n",
    "\n",
    "# 랜덤 서치를 위한 파라미터 분포 설정\n",
    "param_dist = {\n",
    "    'learning_rate': np.logspace(-3, 0, 100),  # 로그 스케일로 학습률 분포 설정\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # 트리의 수\n",
    "    'max_depth': [3, 5, 7, 9],  # 트리의 최대 깊이\n",
    "    'num_leaves': [15, 31, 63, 127],  # 각 트리의 최대 잎의 수\n",
    "    'min_child_samples': [10, 20, 30, 50],  # 각 리프 노드에 필요한 최소 데이터 수\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # 샘플링 비율\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]  # 각 트리를 구성하는 특성의 비율\n",
    "}\n",
    "\n",
    "# 랜덤 서치 수행\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # 시도할 하이퍼파라미터 조합의 수\n",
    "    cv=5,  # 교차 검증 폴드 수\n",
    "    scoring='neg_mean_squared_error',  # 평가 지표\n",
    "    n_jobs=-1,  # 모든 CPU 코어 사용\n",
    "    verbose=2,  # 실행 과정 표시\n",
    "    random_state=42  # 랜덤 시드 고정\n",
    ")\n",
    "\n",
    "# 랜덤 서치 수행\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 및 파라미터 출력\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행 및 성능 평가\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"EditedTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_to_zero = testData[(testData[\"Age\"] < 17) | \n",
    "                      (testData[\"Age\"] > 75) | \n",
    "                      (testData[\"Employment_Status\"] == \"Not Working\") | \n",
    "                      (testData[\"Employment_Status\"] == \"Seeking Full-Time\")].index\n",
    "\n",
    "# 'df_encoded'는 훈련 데이터에 대해 이미 원핫 인코딩이 수행된 DataFrame입니다.\n",
    "# 'train_cols'는 원핫 인코딩된 훈련 데이터의 열 순서입니다.\n",
    "train_cols = edit_train_data.columns\n",
    "\n",
    "# 'columns' 리스트에 지정된 열에 대해 testData 데이터프레임을 원핫 인코딩합니다.\n",
    "edited_testData = pd.get_dummies(testData, columns=columns, dtype=int)\n",
    "\n",
    "# 훈련 데이터에만 있는 열을 찾습니다.\n",
    "missing_cols = set(train_cols) - set(edited_testData.columns)\n",
    "\n",
    "# 훈련 데이터에만 있는 열을 테스트 데이터에 추가하고, 해당 열의 값을 0으로 설정합니다.\n",
    "for col in missing_cols:\n",
    "    edited_testData[col] = 0\n",
    "\n",
    "# 테스트 데이터에서 훈련 데이터에 없는 열을 찾습니다.\n",
    "extra_cols = set(edited_testData.columns) - set(train_cols)\n",
    "\n",
    "# 테스트 데이터에서 훈련 데이터에 없는 열을 제거합니다.\n",
    "edited_testData = edited_testData.drop(columns=extra_cols)\n",
    "\n",
    "# 테스트 데이터의 열 순서를 훈련 데이터의 열 순서와 동일하게 재정렬합니다.\n",
    "edited_testData = edited_testData[train_cols]\n",
    "\n",
    "# 이제 'edited_testData'는 원핫 인코딩된 테스트 데이터이며, 열 순서가 'df_encoded'와 일치합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "y_pred_lightGBM = best_model.predict(edited_testData) \n",
    "for i in range(len(y_pred_lightGBM)):\n",
    "    if i in row_to_zero:\n",
    "        y_pred_lightGBM[i]=0\n",
    "y_pred_lightGBM[y_pred_lightGBM < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID 생성\n",
    "ids = [f'TEST_{i:04d}' for i in range(len(y_pred_lightGBM))]\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Income': y_pred_lightGBM\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv('submissionFilelightGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
