{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본적으로 사용할 라이브러리를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습데이터를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습용데이터에 대한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### column중에 범주형 자료를 담은 column에서 범주를 줄이기 위해 국가를 대륙으로 매핑하는과정에서 필요한 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column중에 범주형 자료를 담은 column에서 범주를 줄이기 위해 국가를 대륙으로 매핑하는과정에서 필요한 딕셔너리\n",
    "country_continent_dict = {\n",
    " 'US': 'US',\n",
    " 'Cuba': 'North America',\n",
    " 'Portugal': 'Europe',\n",
    " 'Mexico': 'North America',\n",
    " 'Unknown': 'US',\n",
    " 'Puerto-Rico': 'North America',\n",
    " 'Germany': 'Europe',\n",
    " 'Japan': 'Asia',\n",
    " 'Poland': 'Europe',\n",
    " 'Columbia': 'South America',\n",
    " 'Philippines': 'Asia',\n",
    " 'Italy': 'Europe',\n",
    " 'Trinadad&Tobago': 'South America',\n",
    " 'England': 'Europe',\n",
    " 'South Korea': 'Asia',\n",
    " 'Iran': 'Asia',\n",
    " 'France': 'Europe',\n",
    " 'India': 'Asia',\n",
    " 'China': 'Asia',\n",
    " 'Dominican-Republic': 'North America',\n",
    " 'Scotland': 'Europe',\n",
    " 'Ecuador': 'South America',\n",
    " 'Nicaragua': 'North America',\n",
    " 'Peru': 'South America',\n",
    " 'Cambodia': 'Asia',\n",
    " 'Canada': 'North America',\n",
    " 'Jamaica': 'North America',\n",
    " 'Vietnam': 'Asia',\n",
    " 'Hong Kong': 'Asia',\n",
    " 'Thailand': 'Asia',\n",
    " 'Haiti': 'North America',\n",
    " 'Guatemala': 'North America',\n",
    " 'Laos': 'Asia',\n",
    " 'Yugoslavia': 'Europe',\n",
    " 'Ireland': 'Europe',\n",
    " 'El-Salvador': 'North America',\n",
    " 'Panama': 'North America',\n",
    " 'Honduras': 'North America',\n",
    " 'Greece': 'Europe',\n",
    " 'Outlying-U S (Guam USVI etc)': 'US',\n",
    " 'Hungary': 'Europe',\n",
    " 'Taiwan': 'Asia',\n",
    " 'Holand-Netherlands': 'Europe'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 메핑을 적용시킬 열: 'Birth_Country', 'Birth_Country (Mother)', 'Birth_Country (Father)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_update = ['Birth_Country', 'Birth_Country (Mother)', 'Birth_Country (Father)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 열에 대해 국가를 대륙으로 매핑\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_update:\n",
    "    train_data[column] = train_data[column].map(country_continent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 독립변수 열들에서 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age 열의 데이터가 17이하 75이상은 제거\n",
    "train_data = train_data[train_data['Age'].between(17, 75)]\n",
    "\n",
    "#'Employment Status'열의 데이터가 'Not Working' 혹은 'Seeking Full-Time'이면 제거\n",
    "train_data = train_data[~train_data['Employment_Status'].isin(['Not Working', 'Seeking Full-Time'])]\n",
    "\n",
    "# 'Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status' 열들을 제거\n",
    "train_data.drop(['Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status'], axis=1, inplace=True)\n",
    "\n",
    "# Map 'Gender' values from 'M' and 'F' to 0 and 1, respectively\n",
    "train_data['Gender'] = train_data['Gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "#ID열은 income에 전혀 영향을 주지 못하므로 drop메소드로 제거한다.\n",
    "edit_train_data = edit_train_data.drop(columns=['ID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### education열에 대한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Education_Status'열에서 범주의 수가 너무 많아 몇개의 범주로 합친다.\n",
    "education_map = {\n",
    "    'High graduate': 'High', 'High Senior': 'High', \n",
    "    'High Junior': 'High', 'High Sophomore': 'High',\n",
    "    'Elementary (5-6)': 'Elementary(1-6)', 'Elementary (1-4)': 'Elementary(1-6)',\n",
    "    'Kindergarten': 'Baby', 'Children': 'Baby'\n",
    "}\n",
    "train_data['Education_Status'] = train_data['Education_Status'].replace(education_map)\n",
    "#원 핫 인코딩을 진행할 범주형 자료를 담은 열들의 리스트\n",
    "columns=[\n",
    "    'Education_Status',\n",
    "    'Employment_Status',\n",
    "    'Industry_Status',\n",
    "    'Occupation_Status',\n",
    "    'Race',\n",
    "    'Hispanic_Origin',\n",
    "    'Martial_Status',\n",
    "    'Household_Summary',\n",
    "    'Citizenship',\n",
    "    'Birth_Country',\n",
    "    'Birth_Country (Father)',\n",
    "    'Birth_Country (Mother)',\n",
    "    'Tax_Status'\n",
    "]\n",
    "#학습데이터를 get_dummies메소드를 이용하여 원 핫 인코딩 한다.\n",
    "edit_train_data=pd.get_dummies(train_data,columns=columns,dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습데이터에서 종속변수열에 대한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시킬 데이터에서 종속변수 income이 이상치(2500이상)를 가지고 있다면 제거\n",
    "train_data = train_data[train_data['Income'] <= 2500]\n",
    "#학습시킬 데이터에 종속변수가 들어있으면 않되므로 drop시킨다.\n",
    "edit_train_data = edit_train_data.drop(columns=['Income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 완성된 학습용 데이터를 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_train_data.to_csv(\"editedTrain.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightGBM모델을 이용하기 위해 최적의 하이퍼파라미터를 찾는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 이용을 위한 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 불러오기 edit_train_data는 'Income'열을 drop했으므로 train_data에서 'Income'열을 가져온다.\n",
    "X = edit_train_data\n",
    "y = train_data['Income']\n",
    "\n",
    "# 위의 불러온 train.csv의 전처리를 거친 데이터 셋에서 학습용과 테스트용으로 데이터를 분리한다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤서치를 통한 하이처파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 랜덤 서치는 하이퍼파라미터 튜닝을 위한 방법 중 하나로, 무작위로 선택된 하이퍼파라미터 조합을 사용하여 모델을 평가한다.\n",
    "##### 그리드 서치는 리드 서치는 사전에 정의된 그리드에서 모든 하이퍼파라미터 조합을 시도하므로 cost가 크다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 모델 생성\n",
    "lgbm_model = LGBMRegressor()\n",
    "#랜덤 서치를 위한 파라미터 분포 설정\n",
    "param_dist = {\n",
    "    'learning_rate': np.logspace(-3, 0, 100),  # 로그 스케일로 학습률 분포 설정\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # 트리의 수\n",
    "    'max_depth': [3, 5, 7, 9],  # 트리의 최대 깊이\n",
    "    'num_leaves': [15, 31, 63, 127],  # 각 트리의 최대 잎의 수\n",
    "    'min_child_samples': [10, 20, 30, 50],  # 각 리프 노드에 필요한 최소 데이터 수\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # 샘플링 비율\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]  # 각 트리를 구성하는 특성의 비율\n",
    "}\n",
    "\n",
    "# 랜덤 서치 수행\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # 시도할 하이퍼파라미터 조합의 수\n",
    "    cv=5,  # 교차 검증 폴드 수\n",
    "    scoring='neg_mean_squared_error',  # 평가 지표\n",
    "    n_jobs=-1,  # 모든 CPU 코어 사용\n",
    "    verbose=2,  # 실행 과정 표시\n",
    "    random_state=42  # 랜덤 시드 고정\n",
    ")\n",
    "\n",
    "# 랜덤 서치 수행\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 및 파라미터 출력\n",
    "print(\"Best Parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터에 대한 예측 수행 및 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "# 성능 평가를 위해 rmse와 R²값을 출력한다.\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data set에 실제 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test.csv를 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 국가를 대륙으로 매핑하는 딕셔너리\n",
    "country_continent_dict = {\n",
    " 'US': 'US',\n",
    " 'Cuba': 'North America',\n",
    " 'Portugal': 'Europe',\n",
    " 'Mexico': 'North America',\n",
    " 'Unknown': 'US',\n",
    " 'Puerto-Rico': 'North America',\n",
    " 'Germany': 'Europe',\n",
    " 'Japan': 'Asia',\n",
    " 'Poland': 'Europe',\n",
    " 'Columbia': 'South America',\n",
    " 'Philippines': 'Asia',\n",
    " 'Italy': 'Europe',\n",
    " 'Trinadad&Tobago': 'South America',\n",
    " 'England': 'Europe',\n",
    " 'South Korea': 'Asia',\n",
    " 'Iran': 'Asia',\n",
    " 'France': 'Europe',\n",
    " 'India': 'Asia',\n",
    " 'China': 'Asia',\n",
    " 'Dominican-Republic': 'North America',\n",
    " 'Scotland': 'Europe',\n",
    " 'Ecuador': 'South America',\n",
    " 'Nicaragua': 'North America',\n",
    " 'Peru': 'South America',\n",
    " 'Cambodia': 'Asia',\n",
    " 'Canada': 'North America',\n",
    " 'Jamaica': 'North America',\n",
    " 'Vietnam': 'Asia',\n",
    " 'Hong Kong': 'Asia',\n",
    " 'Thailand': 'Asia',\n",
    " 'Haiti': 'North America',\n",
    " 'Guatemala': 'North America',\n",
    " 'Laos': 'Asia',\n",
    " 'Yugoslavia': 'Europe',\n",
    " 'Ireland': 'Europe',\n",
    " 'El-Salvador': 'North America',\n",
    " 'Panama': 'North America',\n",
    " 'Honduras': 'North America',\n",
    " 'Greece': 'Europe',\n",
    " 'Outlying-U S (Guam USVI etc)': 'US',\n",
    " 'Hungary': 'Europe',\n",
    " 'Taiwan': 'Asia',\n",
    " 'Holand-Netherlands': 'Europe'\n",
    "}\n",
    "\n",
    "# 업데이트할 열 목록: 본인 출신국가, 엄마 출신국가, 아빠 출신국가\n",
    "columns_to_update = ['Birth_Country', 'Birth_Country (Mother)', 'Birth_Country (Father)']\n",
    "\n",
    "# 각 열에 대해 국가를 대륙으로 매핑\n",
    "for column in columns_to_update:\n",
    "    testData[column] = testData[column].map(country_continent_dict)\n",
    "\n",
    "\n",
    "# Drop the 'Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status' columns\n",
    "testData.drop(['Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status'], axis=1, inplace=True)\n",
    "\n",
    "# Map 'Gender' values from 'M' and 'F' to 0 and 1, respectively\n",
    "testData['Gender'] = testData['Gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "# Consolidate education levels and rename as specified\n",
    "education_map = {\n",
    "    'High graduate': 'High', 'High Senior': 'High', \n",
    "    'High Junior': 'High', 'High Sophomore': 'High',\n",
    "    'Elementary (5-6)': 'Elementary(1-6)', 'Elementary (1-4)': 'Elementary(1-6)',\n",
    "    'Kindergarten': 'Baby', 'Children': 'Baby'\n",
    "}\n",
    "testData['Education_Status'] = testData['Education_Status'].replace(education_map)\n",
    "columns=[\n",
    "    'Education_Status',\n",
    "    'Employment_Status',\n",
    "    'Industry_Status',\n",
    "    'Occupation_Status',\n",
    "    'Race',\n",
    "    'Hispanic_Origin',\n",
    "    'Martial_Status',\n",
    "    'Household_Summary',\n",
    "    'Citizenship',\n",
    "    'Birth_Country',\n",
    "    'Birth_Country (Father)',\n",
    "    'Birth_Country (Mother)',\n",
    "    'Tax_Status'\n",
    "]\n",
    "#제거코드(ID)\n",
    "testData = testData.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test에서는 행을 제거할 수 없으므로 예측이 끝나고 income이 0이 되기로 판정한 행번호 목록을 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_zero = testData[(testData[\"Age\"] < 17) | \n",
    "                      (testData[\"Age\"] > 75) | \n",
    "                      (testData[\"Employment_Status\"] == \"Not Working\") | \n",
    "                      (testData[\"Employment_Status\"] == \"Seeking Full-Time\")].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 범주형 데이터를 담은 열들에 대해 train에는 있고 test에는 없는 범주가 있어 원핫 인코딩 시 열의 갯수가 맞지 않는 에러가 발생하므로 에러처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'df_encoded'는 훈련 데이터에 대해 이미 원핫 인코딩이 수행된 DataFrame이다.\n",
    "# 'train_cols'는 원핫 인코딩된 훈련 데이터의 열 순서이다.\n",
    "train_cols = edit_train_data.columns\n",
    "\n",
    "# 'columns' 리스트에 지정된 열에 대해 testData 데이터프레임을 원핫 인코딩한다.\n",
    "edited_testData = pd.get_dummies(testData, columns=columns, dtype=int)\n",
    "\n",
    "# 훈련 데이터에만 있는 열을 찾는다.\n",
    "missing_cols = set(train_cols) - set(edited_testData.columns)\n",
    "\n",
    "# 훈련 데이터에만 있는 열을 테스트 데이터에 추가하고, 해당 열의 값을 0으로 설정한다.\n",
    "for col in missing_cols:\n",
    "    edited_testData[col] = 0\n",
    "\n",
    "# 테스트 데이터에서 훈련 데이터에 없는 열을 찾는니다.\n",
    "extra_cols = set(edited_testData.columns) - set(train_cols)\n",
    "\n",
    "# 테스트 데이터에서 훈련 데이터에 없는 열을 제거한다.\n",
    "edited_testData = edited_testData.drop(columns=extra_cols)\n",
    "\n",
    "# 테스트 데이터의 열 순서를 훈련 데이터의 열 순서와 동일하게 재정렬한다.\n",
    "edited_testData = edited_testData[train_cols]\n",
    "\n",
    "# 이제 'edited_testData'는 원핫 인코딩된 테스트 데이터이며, 열 순서가 'df_encoded'와 일치한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최종 모델을 이용한 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과에서 이전에 얻어둔 row_to_zero와 예측값이 음수인 행들의 값을 전부 0으로 처리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "y_pred_lightGBM = best_model.predict(edited_testData) \n",
    "for i in range(len(y_pred_lightGBM)):\n",
    "    if i in row_to_zero:\n",
    "        y_pred_lightGBM[i]=0\n",
    "y_pred_lightGBM[y_pred_lightGBM < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출파일 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID 생성\n",
    "ids = [f'TEST_{i:04d}' for i in range(len(y_pred_lightGBM))]\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Income': y_pred_lightGBM\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv('submissionFilelightGBM.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
