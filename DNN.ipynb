{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "train_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 국가를 대륙으로 매핑하는 딕셔너리\n",
    "country_continent_dict = {\n",
    " 'US': 'US',\n",
    " 'Cuba': 'North America',\n",
    " 'Portugal': 'Europe',\n",
    " 'Mexico': 'North America',\n",
    " 'Unknown': 'US',\n",
    " 'Puerto-Rico': 'North America',\n",
    " 'Germany': 'Europe',\n",
    " 'Japan': 'Asia',\n",
    " 'Poland': 'Europe',\n",
    " 'Columbia': 'South America',\n",
    " 'Philippines': 'Asia',\n",
    " 'Italy': 'Europe',\n",
    " 'Trinadad&Tobago': 'South America',\n",
    " 'England': 'Europe',\n",
    " 'South Korea': 'Asia',\n",
    " 'Iran': 'Asia',\n",
    " 'France': 'Europe',\n",
    " 'India': 'Asia',\n",
    " 'China': 'Asia',\n",
    " 'Dominican-Republic': 'North America',\n",
    " 'Scotland': 'Europe',\n",
    " 'Ecuador': 'South America',\n",
    " 'Nicaragua': 'North America',\n",
    " 'Peru': 'South America',\n",
    " 'Cambodia': 'Asia',\n",
    " 'Canada': 'North America',\n",
    " 'Jamaica': 'North America',\n",
    " 'Vietnam': 'Asia',\n",
    " 'Hong Kong': 'Asia',\n",
    " 'Thailand': 'Asia',\n",
    " 'Haiti': 'North America',\n",
    " 'Guatemala': 'North America',\n",
    " 'Laos': 'Asia',\n",
    " 'Yugoslavia': 'Europe',\n",
    " 'Ireland': 'Europe',\n",
    " 'El-Salvador': 'North America',\n",
    " 'Panama': 'North America',\n",
    " 'Honduras': 'North America',\n",
    " 'Greece': 'Europe',\n",
    " 'Outlying-U S (Guam USVI etc)': 'US',\n",
    " 'Hungary': 'Europe',\n",
    " 'Taiwan': 'Asia',\n",
    " 'Holand-Netherlands': 'Europe'\n",
    "}\n",
    "\n",
    "# 업데이트할 열 목록: 본인 출신국가, 엄마 출신국가, 아빠 출신국가\n",
    "columns_to_update = ['Birth_Country', 'Birth_Country (Mother)', 'Birth_Country (Father)']\n",
    "\n",
    "# 각 열에 대해 국가를 대륙으로 매핑\n",
    "for column in columns_to_update:\n",
    "    train_data[column] = train_data[column].map(country_continent_dict)\n",
    "\n",
    "# Remove rows where 'Age' is below 17 or above 75\n",
    "train_data = train_data[train_data['Age'].between(17, 75)]\n",
    "\n",
    "# Remove rows with 'Employment Status' as 'Not Working' or 'Seeking Full-Time'\n",
    "train_data = train_data[~train_data['Employment_Status'].isin(['Not Working', 'Seeking Full-Time'])]\n",
    "\n",
    "# Drop the 'Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status' columns\n",
    "train_data.drop(['Gains', 'Losses', 'Dividends', 'Household_Status', 'Income_Status'], axis=1, inplace=True)\n",
    "\n",
    "# Map 'Gender' values from 'M' and 'F' to 0 and 1, respectively\n",
    "train_data['Gender'] = train_data['Gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "# Consolidate education levels and rename as specified\n",
    "education_map = {\n",
    "    'High graduate': 'High', 'High Senior': 'High', \n",
    "    'High Junior': 'High', 'High Sophomore': 'High',\n",
    "    'Elementary (5-6)': 'Elementary(1-6)', 'Elementary (1-4)': 'Elementary(1-6)',\n",
    "    'Kindergarten': 'Baby', 'Children': 'Baby'\n",
    "}\n",
    "train_data['Education_Status'] = train_data['Education_Status'].replace(education_map)\n",
    "columns=[\n",
    "    'Education_Status',\n",
    "    'Employment_Status',\n",
    "    'Industry_Status',\n",
    "    'Occupation_Status',\n",
    "    'Race',\n",
    "    'Hispanic_Origin',\n",
    "    'Martial_Status',\n",
    "    'Household_Summary',\n",
    "    'Citizenship',\n",
    "    'Birth_Country',\n",
    "    'Birth_Country (Father)',\n",
    "    'Birth_Country (Mother)',\n",
    "    'Tax_Status'\n",
    "]\n",
    "edit_train_data=pd.get_dummies(train_data,columns=columns,dtype=int)\n",
    "edit_train_data = edit_train_data.drop(columns=['ID'])\n",
    "edit_train_data = edit_train_data.drop(columns=['Income'])\n",
    "edit_train_data.to_csv(\"editedTrain.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "ready_X = edit_train_data\n",
    "ready_y = train_data['Income']\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(ready_X, ready_y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "신경망 모델 성능:\n",
      "RMSE: 711.2999790857706\n",
      "R²: 0.076562735537824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 신경망 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))  # 입력층\n",
    "model.add(Dense(32, activation='relu'))  # 은닉층\n",
    "model.add(Dense(1))  # 출력층\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "\n",
    "# 성능 평가\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"신경망 모델 성능:\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 파이프라인 생성 (스케일링 추가)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[1;32m----> 8\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m'\u001b[39m, model)  \u001b[38;5;66;03m# MLPRegressor 모델을 사용합니다.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 새로운 하이퍼파라미터 그리드 생성\u001b[39;00m\n\u001b[0;32m     12\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp__hidden_layer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [(\u001b[38;5;241m50\u001b[39m,), (\u001b[38;5;241m100\u001b[39m,), (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m), (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m50\u001b[39m)],\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp__activation\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp__solver\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp__alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     17\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 파이프라인 생성 (스케일링 추가)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', model)  # MLPRegressor 모델을 사용합니다.\n",
    "])\n",
    "\n",
    "# 새로운 하이퍼파라미터 그리드 생성\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'mlp__activation': ['relu', 'tanh','sigmoid'],\n",
    "    'mlp__solver': ['adam', 'sgd','lbfgs'],\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# 그리드 서치 수행\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"EditedTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_to_zero = testData[(testData[\"Age\"] < 17) | \n",
    "                      (testData[\"Age\"] > 75) | \n",
    "                      (testData[\"Employment_Status\"] == \"Not Working\") | \n",
    "                      (testData[\"Employment_Status\"] == \"Seeking Full-Time\")].index\n",
    "\n",
    "# 'df_encoded'는 훈련 데이터에 대해 이미 원핫 인코딩이 수행된 DataFrame입니다.\n",
    "# 'train_cols'는 원핫 인코딩된 훈련 데이터의 열 순서입니다.\n",
    "train_cols = edit_train_data.columns\n",
    "\n",
    "# 'columns' 리스트에 지정된 열에 대해 testData 데이터프레임을 원핫 인코딩합니다.\n",
    "edited_testData = pd.get_dummies(testData, columns=columns, dtype=int)\n",
    "\n",
    "# 훈련 데이터에만 있는 열을 찾습니다.\n",
    "missing_cols = set(train_cols) - set(edited_testData.columns)\n",
    "\n",
    "# 훈련 데이터에만 있는 열을 테스트 데이터에 추가하고, 해당 열의 값을 0으로 설정합니다.\n",
    "for col in missing_cols:\n",
    "    edited_testData[col] = 0\n",
    "\n",
    "# 테스트 데이터에서 훈련 데이터에 없는 열을 찾습니다.\n",
    "extra_cols = set(edited_testData.columns) - set(train_cols)\n",
    "\n",
    "# 테스트 데이터에서 훈련 데이터에 없는 열을 제거합니다.\n",
    "edited_testData = edited_testData.drop(columns=extra_cols)\n",
    "\n",
    "# 테스트 데이터의 열 순서를 훈련 데이터의 열 순서와 동일하게 재정렬합니다.\n",
    "edited_testData = edited_testData[train_cols]\n",
    "\n",
    "# 이제 'edited_testData'는 원핫 인코딩된 테스트 데이터이며, 열 순서가 'df_encoded'와 일치합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_DNN = grid_search.predict(edited_testData) \n",
    "for i in range(len(y_pred_DNN)):\n",
    "    if i in row_to_zero:\n",
    "        y_pred_DNN[i]=0\n",
    "    if y_pred_DNN<0:\n",
    "        y_pred_DNN[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID 생성\n",
    "ids = [f'TEST_{i:04d}' for i in range(len(y_pred))]\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Income': y_pred_DNN.flatten()\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv('submissionFileDNN.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
